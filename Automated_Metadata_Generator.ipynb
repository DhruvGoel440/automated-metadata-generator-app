{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**AUTOMATED METADATA GENERATOR**"
      ],
      "metadata": {
        "id": "z5dxZv-jGflx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Live App: https://automated-metadata-generator-app.streamlit.app\n",
        "\n",
        "##Video Demo: https://drive.google.com/file/d/1JzmD3WbEOZeGhpquPUpZegwIeKFOzQQf/view?usp=sharing"
      ],
      "metadata": {
        "id": "1OXF7nAbGeye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installing the necesary Libraries and Modules"
      ],
      "metadata": {
        "id": "JO5uxh05hXhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# System-level dependencies for OCR\n",
        "!apt-get update -qq && \\\n",
        "apt-get install -y -qq poppler-utils tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV2peUZjhIWs",
        "outputId": "e31253bc-ec66-4298-8205-162e19314327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet \\\n",
        "    pdfminer.six \\\n",
        "    python-docx \\\n",
        "    pytesseract \\\n",
        "    pillow \\\n",
        "    spacy \\\n",
        "    keybert \\\n",
        "    transformers \\\n",
        "    sentence-transformers \\\n",
        "    sentencepiece \\\n",
        "    matplotlib \\\n",
        "    numpy \\\n",
        "    wordcloud \\\n",
        "    streamlit \\\n",
        "    pyngrok \\\n",
        "    easyocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MVw5XfJKDXu2",
        "outputId": "43d92210-218a-4d25-872e-ad625cc7b1d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rRIHdY9c9RG4",
        "outputId": "172f2bf5-9f8c-45dd-ab96-7c985b44d589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Code for Metadata Generation"
      ],
      "metadata": {
        "id": "CImlzk71hgxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile metadata_gen.py\n",
        "\n",
        "# Import modules\n",
        "import re, io\n",
        "from pdfminer.high_level import extract_text as extract_pdf_text\n",
        "import docx\n",
        "import easyocr\n",
        "from PIL import Image\n",
        "import spacy\n",
        "from keybert import KeyBERT\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Load models\n",
        "nlp_model = spacy.load(\"en_core_web_sm\")\n",
        "keyword_extractor = KeyBERT()\n",
        "text_summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "ocr_reader = easyocr.Reader(['en'], gpu=False)\n",
        "\n",
        "# Function for reading the document uploaded on the app\n",
        "def read_document(path, extension):\n",
        "    try:\n",
        "        if extension == \".pdf\":\n",
        "            return extract_pdf_text(path)\n",
        "        elif extension == \".docx\":\n",
        "            return \"\\n\".join(para.text for para in docx.Document(path).paragraphs)\n",
        "        elif extension in [\".png\", \".jpg\", \".jpeg\"]:\n",
        "            lines = ocr_reader.readtext(path, detail=0)\n",
        "            return \"\\n\".join(lines)\n",
        "        elif extension == \".txt\":\n",
        "            return open(path, encoding=\"utf-8\").read()\n",
        "    except Exception as e:\n",
        "        return f\"Could not read file: {e}\"\n",
        "    return \"\"\n",
        "\n",
        "# Function to find out if the line is a Potential Heading\n",
        "def is_potential_heading(line):\n",
        "    text = line.strip()\n",
        "    return (\n",
        "        0 < len(text) <= 100 and (\n",
        "            text.isupper() or\n",
        "            text.endswith(\":\") or\n",
        "            re.match(r\"^\\d+[\\.\\)]\", text) or\n",
        "            text.lower() in [\n",
        "                \"introduction\", \"background\", \"challenges\", \"limitations\", \"results\",\n",
        "                \"discussion\", \"methodology\", \"methods\", \"conclusion\", \"summary\",\n",
        "                \"references\", \"abstract\", \"future work\"\n",
        "            ] or\n",
        "            len(text.split()) <= 5\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Function to divide the text into sections\n",
        "def segment_text_into_sections(raw_text):\n",
        "    lines = [line for line in raw_text.split(\"\\n\") if line.strip()]\n",
        "    structured = {}\n",
        "    heading, buffer = None, []\n",
        "\n",
        "    for line in lines:\n",
        "        if is_potential_heading(line):\n",
        "            if heading and buffer:\n",
        "                structured[heading] = \"\\n\".join(buffer).strip()\n",
        "            heading, buffer = line.strip(), []\n",
        "        else:\n",
        "            if heading:\n",
        "                buffer.append(line)\n",
        "\n",
        "    if heading and buffer:\n",
        "        structured[heading] = \"\\n\".join(buffer).strip()\n",
        "\n",
        "    if len(structured) < 3 or all(len(sec.split()) < 50 for sec in structured.values()):\n",
        "        paras = [p.strip() for p in raw_text.split(\"\\n\\n\") if len(p.split()) > 30]\n",
        "        if len(paras) < 3:\n",
        "            structured = {\"Main Content\": \"\\n\\n\".join(paras)}\n",
        "        else:\n",
        "            vectors = sbert_model.encode(paras, normalize_embeddings=True)\n",
        "            groups = util.community_detection(vectors, min_community_size=1, threshold=0.75)\n",
        "            structured = {f\"Section {i+1}\": \"\\n\\n\".join(paras[idx] for idx in cluster) for i, cluster in enumerate(groups)}\n",
        "\n",
        "    return structured\n",
        "\n",
        "# Function to extract metadata from the text\n",
        "def extract_metadata(text, fast=True):\n",
        "    sections = segment_text_into_sections(text)\n",
        "    summaries, key_terms, keyword_scores = [], [], {}\n",
        "    named_entities = set()\n",
        "\n",
        "    if not fast:\n",
        "        for title, body in sorted(sections.items(), key=lambda x: len(x[1]), reverse=True)[:3]:\n",
        "            try:\n",
        "                summary = text_summarizer(body[:1024], max_length=120, min_length=30, do_sample=False)[0]['summary_text']\n",
        "                summaries.append(f\"### {title}\\n{summary}\")\n",
        "            except:\n",
        "                continue\n",
        "    else:\n",
        "        try:\n",
        "            summaries.append(text_summarizer(text[:1024], max_length=150, min_length=40, do_sample=False)[0]['summary_text'])\n",
        "        except:\n",
        "            summaries.append(\"Summary generation failed.\")\n",
        "\n",
        "    if len(sections) > 6:\n",
        "        top_keywords = keyword_extractor.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words=\"english\", top_n=10)\n",
        "        key_terms = [term for term, _ in top_keywords]\n",
        "        keyword_scores = dict(top_keywords)\n",
        "        named_entities = {(ent.text, ent.label_) for ent in nlp_model(text).ents}\n",
        "    else:\n",
        "        for _, content in sections.items():\n",
        "            for term, score in keyword_extractor.extract_keywords(content, keyphrase_ngram_range=(1,2), stop_words=\"english\", top_n=3):\n",
        "                if term not in keyword_scores:\n",
        "                    key_terms.append(term)\n",
        "                    keyword_scores[term] = score\n",
        "            named_entities.update((ent.text, ent.label_) for ent in nlp_model(content).ents)\n",
        "\n",
        "    return {\n",
        "        \"summary\": \"\\n\\n\".join(summaries),\n",
        "        \"structured_metadata\": sections,\n",
        "        \"keywords\": key_terms,\n",
        "        \"keyword_scores\": keyword_scores,\n",
        "        \"named_entities\": list(named_entities)\n",
        "    }\n",
        "\n",
        "# Function to get word count, sentence count etc. from the text\n",
        "def get_document_stats(text):\n",
        "    doc = nlp_model(text)\n",
        "    return {\n",
        "        \"word_count\": len(text.split()),\n",
        "        \"sentence_count\": len(list(doc.sents)),\n",
        "        \"entity_count\": len(doc.ents)\n",
        "    }\n",
        "\n",
        "# Function to create the Wordcloud for the document\n",
        "def create_wordcloud_image(text):\n",
        "    return WordCloud(width=800, height=400, background_color=\"white\").generate(text).to_image()\n",
        "\n",
        "# Function to generate the keyword relevance chart\n",
        "def keyword_score_bar_image(scores):\n",
        "    keys, values = list(scores.keys()), list(scores.values())\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    colors = plt.get_cmap(\"Blues\")(plt.Normalize(min(values), max(values))(values))\n",
        "    bars = ax.barh(keys[::-1], values[::-1], color=colors[::-1])\n",
        "    ax.set_title(\"Top Keywords (Relevance)\")\n",
        "    for bar, val in zip(bars, values[::-1]):\n",
        "        ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height() / 2,\n",
        "                f\"{val:.2f}\", va='center', fontsize=9)\n",
        "    plt.tight_layout()\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format=\"png\")\n",
        "    buf.seek(0)\n",
        "    return Image.open(buf)\n",
        "\n",
        "# Function to render NER tags with HTML highlighting\n",
        "def visualize_named_entities(text):\n",
        "    highlight_colors = {\n",
        "        \"ORG\": \"#ffd966\", \"PERSON\": \"#f4cccc\", \"GPE\": \"#c9daf8\",\n",
        "        \"DATE\": \"#d9ead3\", \"MONEY\": \"#e6b8af\", \"PRODUCT\": \"#b4a7d6\", \"EVENT\": \"#a2c4c9\"\n",
        "    }\n",
        "    doc = nlp_model(text)\n",
        "    result_html, last_idx = \"\", 0\n",
        "    for ent in doc.ents:\n",
        "        result_html += text[last_idx:ent.start_char]\n",
        "        shade = highlight_colors.get(ent.label_, \"#e0e0e0\")\n",
        "        result_html += f\"<span style='background:{shade};padding:2px 5px;border-radius:5px;margin:1px;'>{ent.text}<sub style='font-size:10px;color:#333;'>({ent.label_})</sub></span>\"\n",
        "        last_idx = ent.end_char\n",
        "    result_html += text[last_idx:]\n",
        "    return result_html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG6aEfvw4JSn",
        "outputId": "3eb95bf9-6ea8-49bc-94c3-674bca557940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting metadata_gen.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Code for UI"
      ],
      "metadata": {
        "id": "2vTVzsU1hnwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "#Import modules\n",
        "import streamlit as st\n",
        "import os, tempfile, json, hashlib\n",
        "import metadata_gen as mg\n",
        "\n",
        "#Streamlit page setup\n",
        "st.set_page_config(\n",
        "    page_title=\"Metadata Generator\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"collapsed\"\n",
        ")\n",
        "st.markdown(\"# Automated Metadata Generator\")\n",
        "\n",
        "#File uploader\n",
        "uploaded_files = st.file_uploader(\n",
        "    \"📁 Upload documents (PDF, DOCX, TXT, Image)\",\n",
        "    type=[\"pdf\", \"docx\", \"txt\", \"png\", \"jpg\", \"jpeg\"],\n",
        "    accept_multiple_files=True\n",
        ")\n",
        "\n",
        "#Processing the uploaded files\n",
        "if uploaded_files:\n",
        "    for uploaded_file in uploaded_files:\n",
        "        file_ext = os.path.splitext(uploaded_file.name)[1].lower()\n",
        "\n",
        "        tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=file_ext)\n",
        "        tmp_file.write(uploaded_file.read())\n",
        "        tmp_file.close()\n",
        "\n",
        "        #Extracting text and analyzing\n",
        "        raw_text = mg.read_document(tmp_file.name, file_ext)\n",
        "        metadata = mg.extract_metadata(raw_text)\n",
        "        stats = mg.get_document_stats(raw_text)\n",
        "\n",
        "        doc_hash = hashlib.md5(uploaded_file.name.encode()).hexdigest()\n",
        "\n",
        "        st.markdown(\"----\")\n",
        "        st.markdown(f\"### 📝 Document ID: `{doc_hash}`\")\n",
        "        st.metric(\"📄 Filename\", uploaded_file.name)\n",
        "\n",
        "        with st.expander(\"📈 File Details\", expanded=True):\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            col1.metric(\"📦 File Size\", f\"{os.path.getsize(tmp_file.name) / 1024:.2f} KB\")\n",
        "            col2.metric(\"📁 File Type\", \"Document\")\n",
        "            col3.metric(\"🧾 Content Type\", uploaded_file.type)\n",
        "\n",
        "        #Summary and keywords\n",
        "        with st.expander(\"📑 Summary & Keywords\", expanded=True):\n",
        "            st.subheader(\"📝 Summary\")\n",
        "            st.write(metadata[\"summary\"])\n",
        "\n",
        "            st.subheader(\"🔑 Keywords\")\n",
        "            st.markdown(\n",
        "                \"\".join(\n",
        "                    f\"<span style='display:inline-block;background:#e0f0ff;color:#004080;\"\n",
        "                    f\"padding:5px 10px;border-radius:15px;margin:2px;font-size:14px;'>{kw}</span>\"\n",
        "                    for kw in metadata[\"keywords\"]\n",
        "                ),\n",
        "                unsafe_allow_html=True\n",
        "            )\n",
        "\n",
        "        #Basic metrics\n",
        "        with st.expander(\"📈 Document Metrics\", expanded=True):\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            col1.metric(\"Word Count\", stats[\"word_count\"])\n",
        "            col2.metric(\"Sentences\", stats[\"sentence_count\"])\n",
        "            col3.metric(\"Named Entities\", stats[\"entity_count\"])\n",
        "\n",
        "        with st.expander(\"🧾 Structured Metadata\"):\n",
        "            for title, content in metadata[\"structured_metadata\"].items():\n",
        "                st.markdown(f\"#### {title}\")\n",
        "                st.write(content)\n",
        "\n",
        "        #Wordcloud & Keyword bar chart\n",
        "        st.markdown(\"### 🔎 Keyword Insights\")\n",
        "        col_wc, col_bar = st.columns(2)\n",
        "\n",
        "        with col_wc:\n",
        "            st.markdown(\"#### ☁️ Word Cloud\")\n",
        "            st.image(mg.create_wordcloud_image(raw_text).resize((700, 400)))\n",
        "\n",
        "        with col_bar:\n",
        "            st.markdown(\"#### 📊 Keyword Relevance\")\n",
        "            st.image(mg.keyword_score_bar_image(metadata[\"keyword_scores\"]).resize((700, 400)))\n",
        "\n",
        "        with st.expander(\"🧠 Named Entity Recognition\"):\n",
        "            st.markdown(mg.visualize_named_entities(raw_text), unsafe_allow_html=True)\n",
        "\n",
        "        #Download JSON metadata of the document\n",
        "        json_filename = f\"{uploaded_file.name}_metadata.json\"\n",
        "        with open(json_filename, \"w\") as json_out:\n",
        "            json.dump(metadata, json_out, indent=2)\n",
        "        with open(json_filename, \"rb\") as json_in:\n",
        "            st.download_button(\n",
        "                label=\"⬇️ Download Metadata JSON\",\n",
        "                data=json_in,\n",
        "                file_name=json_filename,\n",
        "                mime=\"application/json\"\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LyD6AURkChO",
        "outputId": "29ea66a3-8a89-44f5-ef48-dcc4d2441a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Code for Launching Streamlit App and Live Demo via Ngrok"
      ],
      "metadata": {
        "id": "Vc38Ha62huB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import modules\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.set_auth_token(\"2yoommfgMH2tyH5ZXQIQqheeyKA_7hEH3TrDHUWYwNf5RCRpS\")\n",
        "\n",
        "try:\n",
        "    subprocess.run([\"pkill\", \"streamlit\"], check=False)\n",
        "except Exception:\n",
        "    pass\n",
        "ngrok.kill()\n",
        "\n",
        "print(\"Launching Streamlit app\")\n",
        "streamlit_process = subprocess.Popen([\"streamlit\", \"run\", \"app.py\"])\n",
        "time.sleep(5)\n",
        "\n",
        "# Open ngrok tunnel\n",
        "print(\"Creating public tunnel with ngrok\")\n",
        "public_url = ngrok.connect(8501)\n",
        "\n",
        "print(f\"Click on this link to view live app demo: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIe-xSbIhoQ7",
        "outputId": "1ee05139-f7e8-4f76-96c0-efc90cfbb11c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching Streamlit app\n",
            "Creating public tunnel with ngrok\n",
            "Click on this link to view live app demo: NgrokTunnel: \"https://3519-34-44-183-12.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KABuy0e4R2gU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}